{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1834c-a9dd-4472-8e51-f24fd79fa19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build a random forest classifier to predict the risk of heart disease based on a dataset of patient\n",
    "information. The dataset contains 303 instances with 14 features, including age, sex, chest pain type,\n",
    "resting blood pressure, serum cholesterol, and maximum heart rate achieved.\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming you have the dataset in a Pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Your dataset\n",
    "data = {\n",
    "    'sex': [1, 1, 0, 1, 0, 1, 0, 1, 1],\n",
    "    'cp': [3, 2, 1, 1, 0, 0, 1, 1, 2],\n",
    "    'trestbps': [145, 130, 130, 120, 120, 140, 140, 120, 172],\n",
    "    'chol': [233, 250, 204, 236, 354, 192, 294, 263, 199],\n",
    "    'fbs': [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    'restecg': [0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
    "    'thalach': [150, 187, 172, 178, 163, 148, 153, 173, 162],\n",
    "    'exang': [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    'oldpeak': [2.3, 3.5, 1.4, 0.8, 0.6, 0.4, 1.3, 0, 0.5],\n",
    "    'slope': [0, 0, 2, 2, 2, 1, 1, 2, 2],\n",
    "    'ca': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'thal': [1, 2, 2, 2, 2, 1, 2, 3, 3],\n",
    "    'target': [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "\n",
    "Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\n",
    "numerical features if necessary.\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Assuming 'df' is the original DataFrame containing the provided data\n",
    "new_data = {\n",
    "    'sex': [1, 0, 2, 1, 3, 3, 2, 2, 3, 0],\n",
    "    'cp': [2, 0, 2, 1, 3, 3, 2, 2, 3, 0],\n",
    "    'trestbps': [150, 140, 130, 130, 110, 150, 120, 120, 150, 150],\n",
    "    'chol': [168, 239, 275, 266, 211, 283, 219, 340, 226, 247],\n",
    "    'fbs': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    'restecg': [1, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
    "    'thalach': [174, 160, 139, 171, 144, 162, 158, 172, 114, 171],\n",
    "    'exang': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
    "    'oldpeak': [1.6, 1.2, 0.2, 0.6, 1.8, 1, 1.6, 0, 2.6, 1.5],\n",
    "    'slope': [2, 2, 2, 2, 1, 2, 1, 2, 0, 2],\n",
    "    'ca': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'thal': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "    'target': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "}\n",
    "\n",
    "df_new = pd.DataFrame(new_data)\n",
    "\n",
    "# Label encoding for 'thal' and 'ca'\n",
    "label_encoder = LabelEncoder()\n",
    "df_new['thal'] = label_encoder.fit_transform(df_new['thal'])\n",
    "df_new['ca'] = label_encoder.fit_transform(df_new['ca'])\n",
    "\n",
    "# Feature scaling for numerical features\n",
    "numerical_features = ['trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "scaler = StandardScaler()\n",
    "df_new[numerical_features] = scaler.fit_transform(df_new[numerical_features])\n",
    "\n",
    "# Now 'df_new' is the preprocessed DataFrame\n",
    "print(df_new)\n",
    "\n",
    "Q2. Split the dataset into a training set (70%) and a test set (30%).\n",
    "\n",
    "the same train_test_split function from scikit-learn to split the new dataset into a training set (70%) and a test set (30%). \n",
    "\n",
    "# Assuming 'df_new_samples' is the new dataset\n",
    "new_data_samples = {\n",
    "    'sex': [3, 0, 2, 0, 2, 3, 1, 2, 2, 2],\n",
    "    'cp': [0, 0, 2, 0, 2, 3, 1, 2, 2, 2],\n",
    "    'trestbps': [140, 135, 130, 140, 150, 140, 160, 150, 110, 140],\n",
    "    'chol': [239, 234, 233, 226, 243, 199, 302, 212, 175, 417],\n",
    "    'fbs': [0, 0, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "    'restecg': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "    'thalach': [151, 161, 179, 178, 137, 178, 162, 157, 123, 157],\n",
    "    'exang': [0, 0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
    "    'oldpeak': [1.8, 0.5, 0.4, 0, 1, 1.4, 0.4, 1.6, 0.6, 0.8],\n",
    "    'slope': [2, 1, 2, 2, 1, 2, 2, 2, 2, 2],\n",
    "    'ca': [2, 0, 0, 0, 0, 0, 2, 0, 0, 1],\n",
    "    'thal': [2, 3, 2, 2, 2, 3, 2, 2, 2, 2],\n",
    "    'target': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "}\n",
    "\n",
    "df_new_samples = pd.DataFrame(new_data_samples)\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X_samples = df_new_samples.drop('target', axis=1)\n",
    "y_samples = df_new_samples['target']\n",
    "\n",
    "# Split the new dataset into a training set (70%) and a test set (30%)\n",
    "X_train_samples, X_test_samples, y_train_samples, y_test_samples = train_test_split(\n",
    "    X_samples, y_samples, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Display the shapes of the training and test sets for the new dataset\n",
    "print(\"Training set shape for new samples:\", X_train_samples.shape, y_train_samples.shape)\n",
    "print(\"Test set shape for new samples:\", X_test_samples.shape, y_test_samples.shape)\n",
    "\n",
    "\n",
    "Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each\n",
    "tree. Use the default values for other hyperparameters.\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming 'X_train_samples', 'y_train_samples' are the training set from the new samples\n",
    "# Assuming 'X_test_samples', 'y_test_samples' are the test set from the new samples\n",
    "\n",
    "# Initialize the Random Forest Classifier with 100 trees and a maximum depth of 10\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train_samples, y_train_samples)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_samples = rf_classifier.predict(X_test_samples)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "accuracy_samples = accuracy_score(y_test_samples, y_pred_samples)\n",
    "conf_matrix_samples = confusion_matrix(y_test_samples, y_pred_samples)\n",
    "class_report_samples = classification_report(y_test_samples, y_pred_samples)\n",
    "\n",
    "# Print the results for the new samples\n",
    "print(f\"Accuracy for new samples: {accuracy_samples}\")\n",
    "print(f\"Confusion Matrix for new samples:\\n{conf_matrix_samples}\")\n",
    "print(f\"Classification Report for new samples:\\n{class_report_samples}\")\n",
    "\n",
    "Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming 'X_new_test', 'y_new_test' are the features and labels for the new test set\n",
    "X_new_test = pd.DataFrame({\n",
    "    'sex': [2, 2, 1, 0, 0, 2, 1, 2, 2, 2],\n",
    "    'cp': [0, 2, 1, 0, 0, 2, 1, 2, 2, 2],\n",
    "    'trestbps': [160, 140, 130, 104, 130, 140, 120, 140, 138, 128],\n",
    "    'chol': [360, 308, 245, 208, 264, 321, 325, 235, 257, 216],\n",
    "    'fbs': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    'restecg': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    'thalach': [151, 142, 180, 148, 143, 182, 172, 180, 156, 115],\n",
    "    'exang': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    'oldpeak': [0.8, 1.5, 0.2, 3, 0.4, 0, 0.2, 0, 0, 0],\n",
    "    'slope': [2, 2, 1, 0, 1, 2, 2, 2, 2, 2],\n",
    "    'ca': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'thal': [2, 2, 2, 2, 2, 2, 2, 2, 2, 0],\n",
    "    'target': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "})\n",
    "\n",
    "# Make predictions on the new test set\n",
    "y_pred_new_test = rf_classifier.predict(X_new_test)\n",
    "\n",
    "# Evaluate the performance using metrics\n",
    "accuracy_new_test = accuracy_score(y_new_test, y_pred_new_test)\n",
    "precision_new_test = precision_score(y_new_test, y_pred_new_test)\n",
    "recall_new_test = recall_score(y_new_test, y_pred_new_test)\n",
    "f1_new_test = f1_score(y_new_test, y_pred_new_test)\n",
    "\n",
    "# Print the results for the new test set\n",
    "print(f\"Accuracy for new test set: {accuracy_new_test}\")\n",
    "print(f\"Precision for new test set: {precision_new_test}\")\n",
    "print(f\"Recall for new test set: {recall_new_test}\")\n",
    "print(f\"F1 Score for new test set: {f1_new_test}\")\n",
    "\n",
    "\n",
    "Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\n",
    "disease risk. Visualise the feature importances using a bar chart.\n",
    "\n",
    "1. Load the necessary libraries.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "2. Load the data and convert it into a pandas dataframe.\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/giussepi/heart-disease/master/heart.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "3. Create the features (X) and the target (y) variables.\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "4. Split the data into training and testing sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "5. Create a RandomForestClassifier and fit it to the training data.\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "6. Extract the feature importances and their corresponding feature names.\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "7. Create a DataFrame containing the feature importances and their corresponding feature names.\n",
    "\n",
    "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "Sort the DataFrame in descending order of importance and reset the index.\n",
    "\n",
    "8. feature_importances = feature_importances.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "Visualize the feature importances using a bar chart.\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importances['feature'].iloc[:5], feature_importances['importance'].iloc[:5], align='center')\n",
    "plt.title('Top 5 Important Features for Predicting Heart Disease Risk')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "In this visualization, the features are listed in descending order of importance. The top 5 features are: age, sex, cholesterol, restecg, and restbp. These features play a significant role in predicting heart disease risk.\n",
    "\n",
    "\n",
    "Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try\n",
    "different values of the number of trees, maximum depth, minimum samples split, and minimum samples\n",
    "leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "Q7. Report the best set of hyperparameters found by the search and the corresponding performance\n",
    "metrics. Compare the performance of the tuned model with the default model.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf_classifier_default = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_classifier_default, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_samples, y_train_samples)\n",
    "\n",
    "# Get the best set of hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best set of hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Evaluate the performance of the tuned model on the test set\n",
    "y_pred_tuned = grid_search.predict(X_test_samples)\n",
    "\n",
    "# Calculate performance metrics for the tuned model\n",
    "accuracy_tuned = accuracy_score(y_test_samples, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test_samples, y_pred_tuned)\n",
    "recall_tuned = recall_score(y_test_samples, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test_samples, y_pred_tuned)\n",
    "\n",
    "# Print the performance metrics for the tuned model\n",
    "print(\"\\nPerformance Metrics for the Tuned Model:\")\n",
    "print(f\"Accuracy: {accuracy_tuned}\")\n",
    "print(f\"Precision: {precision_tuned}\")\n",
    "print(f\"Recall: {recall_tuned}\")\n",
    "print(f\"F1 Score: {f1_tuned}\")\n",
    "\n",
    "# Compare with the default model\n",
    "y_pred_default = rf_classifier_default.predict(X_test_samples)\n",
    "accuracy_default = accuracy_score(y_test_samples, y_pred_default)\n",
    "\n",
    "print(\"\\nPerformance Metrics for the Default Model:\")\n",
    "print(f\"Accuracy: {accuracy_default}\")\n",
    "\n",
    "\n",
    "\n",
    "Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot the\n",
    "decision boundaries on a scatter plot of two of the most important features. Discuss the insights and\n",
    "limitations of the model for predicting heart disease risk.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming 'X_samples' contains the features of the new samples\n",
    "# Assuming 'y_samples' contains the corresponding labels\n",
    "\n",
    "# Select the top two features based on feature importance\n",
    "top_features = feature_importance_df.head(2)['Feature'].values\n",
    "\n",
    "# Extract the selected features for visualization\n",
    "X_visualization = X_samples[top_features]\n",
    "\n",
    "# Standardize the data for better visualization\n",
    "scaler = StandardScaler()\n",
    "X_visualization_scaled = scaler.fit_transform(X_visualization)\n",
    "\n",
    "# Reduce dimensionality using PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_visualization_scaled)\n",
    "\n",
    "# Fit a Random Forest Classifier on the entire dataset\n",
    "rf_classifier_visualization = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_classifier_visualization.fit(X_visualization_scaled, y_samples)\n",
    "\n",
    "# Plot decision boundaries on a scatter plot\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = rf_classifier_visualization.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_samples, edgecolors='k', cmap=plt.cm.coolwarm)\n",
    "plt.xlabel(top_features[0])\n",
    "plt.ylabel(top_features[1])\n",
    "plt.title('Decision Boundaries of Random Forest Classifier')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
